log_dir: "./logs"
wandb: False
project: "V3"
name: "insnotes_trial"
debug: False
random_seed: 

dataloader: "librispeech_h5_dataloader"
data_dir: "../data/Librispeech100_h5"
method: "V3"

device: "cuda"
precision: "bfloat16" # float32, float16 or bfloat16
num_workers: 4
batch_size: 32
epochs: 10000 # incremental if load_checkpoint is given
val_every_n_epochs: 1
log_every_n_steps: 1
save_top_k: 3 # based on validation loss
save_every_n_epochs: 1 # larger or equal to val_frequency

load_checkpoint: None
active_checkpoint: None

loss_config:
  weights:
    recon_loss: 1
    content_loss: 1
    style_loss: 1
    sample_loss: 1
    fragment_loss: 1
    commit_loss: 0.1
  relativity: 5
  supersample_content: True
  widen_style: 3

model_config:
  n_channels: 256
  n_fragments: 8
  fragment_len: 64 # determined by dataloader
  n_feature: 80 # determined by dataloader
  d_emb_c: 512
  d_emb_s: 512
  n_atoms: 80
  threshold_ema_dead_code: 0
  vq_ema_decay: 0.95


optimizer_config:
  optimizer: "AdamW"
  lr: 1.0e-3
  beta1: 0.9
  beta2: 0.999
  eps: 1.0e-8
  momentum: 0.9
  weight_decay: 0.1

  scheduler: "exponential_decay" # "cosine_annealing", "exponential_decay"
  lr_anneal_epochs: 100
  lr_anneal_min_factor: 0.01
  lr_decay_factor: 0.98
  lr_decay_epochs: 20
  lr_decay_min_factor: 0.02
  warmup_epochs: 0
  warmup_factor: 0.1